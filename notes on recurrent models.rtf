{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red234\green233\blue255;\red135\green135\blue135;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\cssrgb\c93333\c93333\c100000;\cssrgb\c60000\c60000\c60000;
}
\margl1440\margr1440\vieww10800\viewh13700\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs26 \cf0 Notes on recurrent network papers\
3/21/2018\
\
Andrej Karpathy blog post\
{\field{\*\fldinst{HYPERLINK "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"}}{\fldrslt http://karpathy.github.io/2015/05/21/rnn-effectiveness/}}\
\
\pard\pardeftab720\sl340\partightenfactor0
\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2     \cf4 \strokec4 self\cf2 \strokec2 .h = np.tanh(np.dot(\cf4 \strokec4 self\cf2 \strokec2 .W_hh, \cf4 \strokec4 self\cf2 \strokec2 .h) + np.dot(\cf4 \strokec4 self\cf2 \strokec2 .W_xh, x))\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \cb1 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\
Hidden unit activation function does not break out biases separately from weights\
\
Input character represented by a one-hot vector (64 wide)\
Output layer is a one-hot vector\
hidden layer takes full hidden layer plus input layer\
Uses softmax classifier on output\
\
\
\
\
\
\
\
\
CTC Networks and Language Models: Prefix Beam Search Explained\
{\field{\*\fldinst{HYPERLINK "https://medium.com/corti-ai/ctc-networks-and-language-models-prefix-beam-search-explained-c11d1ee23306"}}{\fldrslt https://medium.com/corti-ai/ctc-networks-and-language-models-prefix-beam-search-explained-c11d1ee23306}}\
By Lasse Borgholt at corti.ai\
\
Automatic Speech Recognition (ASR) uses Connectionist Temporal Classification (CTC) \
\
No longer need to segment the audio into sound chunks\
To make this competitive with prior methods introduce Prefix Beam Search, use a language model to rectify the results of the network\
\
Language model: given a sentence as input, give probability of last word as output\
Max or green decoding: just take char with highest pr\
\'97 collapse repeating characters\
\'97 discard blank tokens\
\
Prefix beam search\
(Basic python implementation here {\field{\*\fldinst{HYPERLINK "https://github.com/corticph/prefix-beam-search/"}}{\fldrslt https://github.com/corticph/prefix-beam-search/}})\
Extend a string from empty to full, iterate at each character (each time step)\
At each character extend multiple prefixes with the most likely characters at that time step\
If we extend with a space character than apply a language model to the last word\
Finally select the most probable prefix overall\
\
Apply a language model means force it to choose a word with high language model probability\
\
First-Pass Large Vocabulary Continuous Speech\
Recognition using Bi-Directional Recurrent DNNs\
Jannun, Maas, Jurafsky and Ng, Stanford\
\
Emission probability ctc[t][c]\
Probability of character c at time step t, just the network output y at that tilmestep\
\
Blank probability Pb[t][l]\
Pr that prefix l originates from a path that ends in a blank character\
Eg prefix \'93B\'94 paths \'93B\'97\'93, \'93-B-\'93, \'93BB-\'93\
\
Nonblank probability Pnb[t][l]\
Eg prefix \'93B\'94 paths \'93\'97B\'94, \'93-BB\'94, \'93BBB\'94\
\
Hyperparameters:\
Alpha - weight of the language model\
Beta - compensation term\
K - width of beam search (how many prefixes to consider at once)\
\
Language model is a function that takes a string and return a probability\
\
\
\
\
\
\
\
\pard\pardeftab720\sl280\partightenfactor0
\cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Deep Neural Language Models for Machine Translation\
Long, Kayser and manning (stanford)\
\
Abstract: more hidden layers in an NLM gives better accuracy\
\
\
\
\
\
\
\
\
\
GloVe: Global Vectors for Word Representation\
Pennington, Socher, Manning (Stanford)\
\pard\pardeftab720\sl280\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://nlp.stanford.edu/projects/glove/"}}{\fldrslt \cf2 https://nlp.stanford.edu/projects/glove/}}\
\
\pard\pardeftab720\sl280\partightenfactor0
\cf2 Unsupervised learning to obtain vector representations for words\
Pre-trained word vectors are available to download\
Acknowledge this if you use them\
Euclidean distance is a good metric\
Predict pr of next word given previous word\
\
*********************************************\
\
Summary:\
Use beam search during recurrent network performance or inversion\
Use GloVe for the language model\
\
}